# Structured Output Evaluation Benchmark for LLMs

## Overview

This repository hosts the code and resources for the "Are LLMs Good at Structured Outputs? A Benchmark for Evaluating Structured Output Capabilities in LLMs" project. Our work introduces a novel benchmark aimed at assessing LLMs' ability to produce structured outputs, such as lists, JSON, and XML. This evaluation is crucial for applications requiring structured data processing and offers insights into the practical usability of LLMs in various domains.

## Highlights

- **Theoretical Foundation**: Analysis of prompt structures through causal graph analysis, providing a scalable and practical evaluation framework.
- **SoEval Dataset**: First dedicated dataset for structured output evaluation, including 13 structured output tasks across 20 different subjects.
- **LLM Evaluation**: Insights into the structured output capabilities of major LLMs, including GPT-4, with baseline results for future research and development.

## License

Distributed under the MIT License. See `LICENSE` for more information.

## Contact

Yu Liu - [ liuyuforwh@gmail.com](mailto: liuyuforwh@gmail.com)


